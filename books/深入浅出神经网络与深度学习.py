# 深入浅出神经网络与深度学习
  前言
第1章 使用神经网络识别手写数字 1
  1.1 感知机 2
  1.2 sigmoid神经元 7
  1.3 神经网络的架构 11
  1.4 一个简单的神经网络：分类手写数字 13
  1.5 利用梯度下降算法进行学习 17
  1.6 实现分类数字的神经网络 25
  1.7 迈向深度学习 37
第2章 反向传播算法工作原理 41
  2.1 热身：使用矩阵快速计算输出 41
  2.2 关于代价函数的两个假设 43
  2.3 阿达马积s⊙t 45
  2.4 反向传播的4个基本方程 45
  2.5 基本方程的证明（选学） 50
  2.6 反向传播算法 51
  2.7 反向传播代码 53
  2.8 就何而言，反向传播算快 55
  2.9 反向传播：全局观 56
第3章 改进神经网络的学习方法 60
  3.1 交叉熵代价函数 60
  3.1.1 引入交叉熵代价函数 64
  3.1.2 使用交叉熵来对MNIST数字进行分类 71
  3.1.3 交叉熵的含义与起源 72
  3.1.4 softmax 74
  3.2 过拟合和正则化 78
  3.2.1 正则化 84
  3.2.2 为何正则化有助于减轻过拟合 89
  3.2.3 其他正则化技术 93
  3.3 权重初始化 102
  3.4 复探手写识别问题：代码 106
  3.5 如何选择神经网络的超参数 116
  3.6 其他技术 126
  3.6.1 随机梯度下降算法的变化形式 126
  3.6.2 其他人工神经元模型 129
  3.6.3 有关神经网络的故事 132
第4章 神经网络可以计算任何函数的可视化证明 134
  4.1 两个预先声明 136
  4.2 一个输入和一个输出的普遍性 137
  4.3 多个输入变量 146
  4.4 不止sigmoid神经元 154
  4.5 修补阶跃函数 156
  4.6 小结 159
第5章 为何深度神经网络很难训练 160
  5.1 梯度消失问题 163
  5.2 梯度消失的原因 168
  5.2.1 为何出现梯度消失 170
  5.2.2 梯度爆炸问题 171
  5.2.3 梯度不稳定问题 172
  5.2.4 梯度消失问题普遍存在 172
  5.3 复杂神经网络中的梯度不稳定 173
  5.4 深度学习的其他障碍 174
第6章 深度学习 175
  6.1 卷积神经网络入门 176
  6.1.1 局部感受野 178
  6.1.2 共享权重和偏置 180
  6.1.3 池化层 182
  6.2 卷积神经网络的实际应用 184
  6.2.1 使用修正线性单元 188
  6.2.2 扩展训练数据 189
  6.2.3 插入额外的全连接层 191
  6.2.4 集成神经网络 192
  6.3 卷积神经网络的代码 195
  6.4 图像识别领域近期的进展 208
  6.4.1 2012年的LRMD论文 208
  6.4.2 2012年的KSH论文 209
  6.4.3 2014年的ILSVRC竞赛 211
  6.4.4 其他活动 212
  6.5 其他深度学习模型 214
  6.5.1 循环神经网络 214
  6.5.2 长短期记忆单元 216
  6.5.3 深度信念网络、生成模型和玻尔兹曼机 216
  6.5.4 其他想法 217
  6.6 神经网络的未来 217
  6.6.1 意图驱动的用户界面 217
  6.6.2 机器学习、数据科学和创新的循环 218
  6.6.3 神经网络和深度学习的作用 218
  6.6.4 神经网络和深度学习将主导人工智能 219
  附录 是否存在关于智能的简单算法 222
